{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "new",
      "language": "python",
      "name": "new"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "FaceDetection_ResnetNetwork",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leekate/college_project_DeepLearning/blob/main/FaceDetection_ResnetNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zZu5d5Mkt4t"
      },
      "source": [
        "# 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDKnQvmbkt4u"
      },
      "source": [
        "import numpy as np\n",
        "x_train = np.load('./Train_np/X_train.npy')\n",
        "y_train = np.load('./Train_np/y_train.npy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3pPU2-mkt4v",
        "outputId": "f4125250-d800-41db-a176-1dd6651ae28d"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(197012, 218, 178)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ANl-yhnkt4w",
        "outputId": "ab14a787-820c-4ee9-ea49-ed94a47d610b"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(197012, 136)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE34xM-Xkt4w"
      },
      "source": [
        "# 정규화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8wLFx06kt4x"
      },
      "source": [
        "#normalization\n",
        "x_train=(x_train.astype(np.float32))/x_train.max()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sio-f6X1kt4x"
      },
      "source": [
        "x_train=np.expand_dims(x_train,-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVOY5nEekt4y",
        "outputId": "7a7c6449-2ae6-4443-8488-fadbcd37f367"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(197012, 218, 178, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH59Ef-Kkt4y"
      },
      "source": [
        "# 데이터 split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNzoZFaakt4y"
      },
      "source": [
        "#importing all neccessary packages\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cnHsRC2kt4z"
      },
      "source": [
        "# 데이터(X)만 넣었을 경우\n",
        "#x_train, x_test = train_test_split(x, test_size=0.2, random_state=123)\n",
        "\n",
        "\n",
        "# 데이터(X)와 레이블(Y)을 넣었을 경우\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=321)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-TOYztJkt4z"
      },
      "source": [
        "x_test=x_test.reshape(1332,1*256*256)\n",
        "transformer.fit(x_test)\n",
        "\n",
        "#MinMaxScaler 모델에 x_train_df 데이터 적용 (최소값, 최대값 계산)\n",
        "\n",
        "x_test = transformer.transform(x_test)\n",
        "print(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3WcSbxKkt4z"
      },
      "source": [
        "#from tensorflow.python.keras.applications import ResNet50\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.python.keras.preprocessing.image import load_img, img_to_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AItf_vfYkt4z"
      },
      "source": [
        "#Resnet디폴트 인풋 사이즈는 224x224\n",
        "#근데 우리는 256,256으로 가자\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import os\n",
        "\n",
        "save_dir=\"./save_model\"\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.mkdir(save_dir)\n",
        "    \n",
        "checkpointer=ModelCheckpoint(os.path.join(save_dir,'{epoch:0.3d}_valloss{val_loss:0.4f}.hdf5'),\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "es=EarlyStopping(monitor='val_loss',mode='min', baseline=0.1)\n",
        "\n",
        "\n",
        "class createResnetModel:\n",
        "    '''\n",
        "    to create Resnet model\n",
        "    using built-in functions of keras such as\n",
        "    conv2D,\n",
        "    maxpooling2D,\n",
        "    flatten,\n",
        "    dropout\n",
        "    '''\n",
        "\n",
        "    def __init__(self,input_size,output_size,epochs,batch_size):\n",
        "        \n",
        "        self.input_size = input_shape\n",
        "        self.output_size = output_size\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.createmodel()\n",
        "\n",
        "\n",
        "    def createmodel(self):\n",
        "        self.model = Sequential()\n",
        "\n",
        "#         self.model.add(ResNet50(include_top=False, \n",
        "#                                 pooling='avg',\n",
        "#                                weights=None))\n",
        "        #model.add(ResNet50(include_top=False, \n",
        "                            #pooling='avg', \n",
        "                            #weights=resnet_weights_path))\n",
        "        #weights: None (임의의 초기값 설정) \n",
        "        #혹은 'imagenet' (ImageNet에 대한 선행 학습) 중 하나.\n",
        "        \n",
        "        self.model.add(ResNet50(include_top=True, \n",
        "                                pooling='avg',\n",
        "                                input_shape=(218,178,1),\n",
        "                               weights=None))\n",
        "        \n",
        "        ################################DROPOUT 0.1로 10TIMES####################################\n",
        "        self.model.add(Flatten())\n",
        "        self.model.add(BatchNormalization())\n",
        "        self.model.add(Dense(2048, activation='relu'))\n",
        "        self.model.add(Dropout(0.3))\n",
        "        \n",
        "        self.model.add(BatchNormalization())\n",
        "        self.model.add(Dense(2048, activation='relu'))\n",
        "        self.model.add(Dropout(0.3))\n",
        "        \n",
        "        self.model.add(BatchNormalization())\n",
        "        self.model.add(Dense(2048, activation='relu'))\n",
        "        self.model.add(Dropout(0.3))\n",
        "        \n",
        "        self.model.add(BatchNormalization())\n",
        "        self.model.add(Dense(1024, activation='relu'))\n",
        "        self.model.add(Dropout(0.3))\n",
        "        \n",
        "        self.model.add(BatchNormalization())\n",
        "        self.model.add(Dense(1024, activation='relu'))\n",
        "        self.model.add(Dropout(0.3))\n",
        "\n",
        "        self.model.add(BatchNormalization())\n",
        "        self.model.add(Dense(1024, activation='relu'))\n",
        "        self.model.add(Dropout(0.3))\n",
        "\n",
        "        self.model.add(BatchNormalization())\n",
        "        self.model.add(Dense(136, activation='softmax'))\n",
        "\n",
        "        self.model.layers[0].trainable = False\n",
        "\n",
        "\n",
        "    \n",
        "    def summary(self):\n",
        "        \n",
        "        return self.model.summary()\n",
        "    \n",
        "    def compile(self,x_train, x_test, y_train, y_test,save= True):\n",
        "        \n",
        "        # Optimiser: the Optimiser  used to reduce the cost calculated by categorical_crossentropy\n",
        "        # Loss: the loss function used to calculate the error\n",
        "        # Metrics: the metrics used to represent the efficiency of the model\n",
        "        # lr : learing rates\n",
        "        \n",
        "        adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "        \n",
        "        self.model.compile(optimizer = adam,loss = 'mean_squared_error', metrics = ['acc'])\n",
        "        self.model.fit(x = x_train, y= y_train,\n",
        "                       batch_size= 64, \n",
        "                       verbose=1, \n",
        "                       epochs=100,\n",
        "                       validation_data= (x_test,y_test),\n",
        "                      callbacks=[checkpointer,es])\n",
        "        if save:\n",
        "            self.save()\n",
        "\n",
        "    def save(self):\n",
        "        \n",
        "        # creates a HDF5 fil 'my_model.h5'\n",
        "        self.model.save('my_Resnetmodel_10000.h5')  \n",
        "        print(\"saved sucessfully\")\n",
        "        \n",
        "    def save_history(self):\n",
        "        \n",
        "        # for visualizing losses and accuracy\n",
        "        # History.history attribute is a record of training loss values\n",
        "        # metrics values at successive epochs \n",
        "        # as well as validation loss values and validation metrics values\n",
        "        train_loss=self.model.history['loss']\n",
        "        val_loss=self.model.history['val_loss']\n",
        "        train_acc=self.model.history['acc']\n",
        "        val_acc=self.model.history['val_acc']\n",
        "        xc=range(self.epochs)\n",
        "        model_history = [\n",
        "                train_loss,\n",
        "                val_loss,\n",
        "                train_acc,\n",
        "                val_acc,\n",
        "                xc]\n",
        "        \n",
        "        np.save('model_Resnet_history.npy',model_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-B5QcJEkt40"
      },
      "source": [
        "\n",
        "####################################################################################################################################\n",
        "#pre-processing all data   \n",
        "#원래:\n",
        "#x_train = np.array(x_train).reshape(-1, 256,256,1).astype('float32')\n",
        "\n",
        "#converting int datasets into float datasets \n",
        "# for easy processing by CPU/GPU\n",
        "#x_train /= 255\n",
        "x= np.median(y_train)\n",
        "y_train = (y_train - x)/ x\n",
        "\n",
        "# train_data /= 255\n",
        "# x= np.median(ytrain)\n",
        "# ytrain = (ytrain - x)/ x\n",
        "\n",
        "#saving rows,col,dim of datasets to input_shape variable\n",
        "#to give input size /shape to CNN model \n",
        "input_shape = x_train.shape[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUIhfEgXkt40",
        "outputId": "b5994cb2-1699-417c-b1fa-aacacaba01bc"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(157609, 218, 178, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMfUhY76kt41",
        "outputId": "f02ad429-7645-438a-cf72-079b8c5b0104"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39403, 218, 178, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5-o9x9gkt41"
      },
      "source": [
        "#pre-processing all data   \n",
        "#원래:\n",
        "#x_test = np.array(x_test).reshape(-1, 256,256,1).astype('float32')\n",
        "\n",
        "#converting int datasets into float datasets \n",
        "# for easy processing by CPU/GPU\n",
        "#x_test /= 255\n",
        "x= np.median(y_test)\n",
        "y_test = (y_test - x)/ x\n",
        "\n",
        "# train_data /= 255\n",
        "# x= np.median(ytrain)\n",
        "# ytrain = (ytrain - x)/ x\n",
        "\n",
        "#saving rows,col,dim of datasets to input_shape variable\n",
        "#to give input size /shape to CNN model \n",
        "input_shape = x_test.shape[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVTBtPXTkt41",
        "outputId": "05b1cc42-aa3e-4a0b-8cf0-3a107cecbc4b"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39403, 218, 178, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DEdfeLNzkt42",
        "outputId": "fe4056cd-5613-46f1-af99-dab09d735b1e"
      },
      "source": [
        "\n",
        "#model = createResnetModel((256,256),(256,256),700,64)\n",
        "model = createResnetModel((218, 178),(136, ),700,64)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Model)             (None, 1000)              25630440  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2048)              2050048   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 2048)              8192      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 2048)              8192      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2048)              8192      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 136)               139400    \n",
            "=================================================================\n",
            "Total params: 40,450,832\n",
            "Trainable params: 14,799,960\n",
            "Non-trainable params: 25,650,872\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQuxSS7Akt42",
        "outputId": "17b82c47-167d-48cd-a3a5-d53aaa5ad2b0"
      },
      "source": [
        "model.compile(x_train, x_test, y_train, y_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 157609 samples, validate on 39403 samples\n",
            "Epoch 1/100\n",
            "157568/157609 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9117"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Precision not allowed in integer format specifier",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-20-5a3ddd7c12fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-10-05893946d99c>\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, x_train, x_test, y_train, y_test, save)\u001b[0m\n\u001b[0;32m    105\u001b[0m                        \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                       callbacks=[checkpointer,es])\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: Precision not allowed in integer format specifier"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrjqZ4Qpkt43"
      },
      "source": [
        "# model.save()\n",
        "# model.save_history()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR5_dJ6pkt43"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}